% !Mode:: "TeX:UTF-8"

\chapter{实验过程与结果分析}
\label{experiment}
本章将介绍本项目的实验部分，包括对KITTI数据集、数据预处理、模型训练以及实验结果分析等内容。在结果分析中，我们首先比较DODT各模块对结果的影响，以考察每个模块的有效性；然后我们探讨了关键帧的选取步长对三维物体检测结果的影响，以便确定最优的步长；最后我们也测试了DODT在多目标追踪任务上的性能，并与前沿方法对比。结果显示DODT框架能很好的完成流数据的三维物体检测以及多目标跟踪任务。

\section{KITTI数据集介绍}
\label{kitti}
本项目的所有实验都是基于无人驾驶领域中广泛使用的KITTI公开数据集开展的。KITTI数据集是由德国卡尔斯鲁厄理工学院和丰田美国技术研究院联合采集的，该数据集包含多种传感器数据：一个惯性导航系统（GPS/IMU，型号为OXTS RT 3003）数据，一个激光雷达（Velodyne HDL-64E）数据，两个灰度相机数据（140万像素）以及两个彩色相机数据（140万像素）。其中激光雷达扫描频率为10帧/秒，相机基本与地平面保持平行，图像采集的尺寸为$1382 \times 512$像素。所有传感器的整体布局如\figurename \ref{fig:KITTI}。

\input{figures/chap04_kitti_car.tex}

KITTI数据集根据不同的任务分为stero、flow、scenceflow、depth、odometry、object以及tracking等部分，对应着场景流估计、深度估计、路径规划、物体检测以及目标追踪等任务。每一个任务数据包都包含了海量的训练数据以及测试数据，供研究者使用。本项目主要使用了KITTI数据集的tracking数据包，该数据包由21段训练视频流（共8004帧数据）以及29段测试视频流（共11095帧数据）组成。每一段视频流都是由连续的RGB图像帧以及三维点云帧组成，在训练数据集中，还包含了每一帧数据对应的二维目标框（针对图像数据）以及三维目标框（针对点云数据）。此外，针对每一段视频流，KITTI还提供了传感器的标定信息以及每一帧的GPS/IMU数据，供研究者数据标定时使用。

多目标追踪的标签共有10项，记录了帧的信息以及帧中每个目标的信息。信息列举如下：
\begin{itemize}
	\item frame id：帧的编号；
	\item object id：每一帧中目标的编号，也是轨迹的编号；
	\item type：目标的类别，有”Car“，”Van“， "Trunk"，”Pedestrian“，”Cyclist“等类别，本实验将”Car“和”Van“合并为”Car“类，并只针对”Car“类进行检测和追踪；
	\item truncated：标记目标是否被图像边界截断，”0“表示不截断，”1“表示截断；
	\item occluded：标记目标被遮挡的程度，共有0到3四个取值，”0“表示完全可视，”1“表示部分遮挡，”2“表示大部分遮挡，”3“表示完全遮挡；
	\item alpha：目标的观测角，$\alpha \in [-\pi, \pi]$；
	\item bbox：物体在图像上的2D边界框，包含左上角，右下角的坐标值；
	\item dimensions：物体的高、框和长，单位为米；
	\item location：物体底部中心点在相机坐标系的三维坐标 x，y，z，单位为米；
	\item ry：物体在相机坐标系沿Y轴的旋转角，$r_y \in [-pi, pi]$。
\end{itemize}
其中目标的观测角$\alpha = -[(\pi+r_y) + (\pi+\beta)]$，$r_y$与$\beta$如\figurename \ref{fig:kitti_obj}所示，而目标的location坐标如\figurename \ref{fig:kitti_box3d}所示。
\input{figures/chap04_kitti_view.tex}

传感器的标定信息保存在”calib.txt“文件中，其中包含了相机的参数矩阵以及各传感器之间的旋转矩阵。信息列举如下：
\begin{itemize}
	\item P0-P3：四个相机的内参矩阵 $P \in \mathcal{R}^{3\times 4}$；
	\item R0\_rect：$\mathcal{R}^{3\times 3}$，将摄像机坐标系转换到图像坐标系的校准矩阵；
	\item Tr\_velo\_to\_cam：$\mathcal{R}^{3\times 4}$，激光雷达坐标系到摄像机坐标系的旋转矩阵；
	\item Tr\_imu\_to\_velo：$\mathcal{R}^{3\times 4}$，IMU坐标系到激光雷达坐标系的旋转矩阵。
\end{itemize}

GPS/IMU数据提供了30项信息，其中包含每一帧中自身车辆的经纬度、海拔、三个欧拉角（roll，yaw以及pitch）、速度、加速度、角速度等信息。本实验主要使用到了经纬度以及欧拉角信息将不同帧之间的信息校准到同一坐标系。欧拉角包含了偏航角（yaw，表示机体轴在水平面上的投影与地轴之间的夹角，右偏为正）、俯仰角（pitch，表示机体轴与地平面之间的夹角，抬头为正） 以及翻滚角（roll，表示机体对称面绕机轴转动的角度，右滚为正），如\figurename \ref{fig:euler}所示。

\input{figures/chap04_euler.tex}


\section{数据预处理}
\label{preprocessing}
DODT框架融合了点云数据信息以及RGB图像数据信息，由于激光雷达的视野和摄像机的视野不同，因此在融合之前需要将激光雷达坐标系下的点云转换到图像坐标系中。假设激光雷达坐标系下点云的齐次坐标为$P_{pc}=[X,Y,Z,1]^T$，其中$X=(x_1,x_2,...,x_n),Y=(y_1,y_2,...,y_n),Z=(z_1,z_2,...,z_n)$，表示点云中所有$n$个点的坐标向量。点云中所有点对应到图像坐标系上的像素点齐次坐标为$P_{img} = [U,V,1]^T$（U，V同样为坐标向量），则转换关系如公式\ref{con:transform}所示。其中$P_{cam}$为相机的参数矩阵，R0\_rect 以及 Tr\_velo\_to\_cam为上一小节校准文件中的转换矩阵。
\begin{equation}
P_{img} = P_{cam} * R0\_rect * Tr\_velo\_to\_cam * P_{pc}
\label{con:transform}
\end{equation}
经过这步转换后，点云与图像都在相同坐标系下。之后我们对点云的预处理方式和AVOD\cite{ku2018joint}一样，首先将落在图像尺寸之外的点过滤掉，然后分别沿X，Z，Y轴截取$[-40,40] \times [0,70] \times [0,2.5]$米的区域作为最终的点云数据。

为了让DODT输入的两帧关键帧数据能够很好的融合从而得到时序信息，我们还需要将两帧数据校准到同一坐标系，该过程需要用到上文提及的GPS/IMU数据。假设DODT输入的相邻关键帧为$F_1$和$F_2$，$F_1$对应时刻车体的经纬度为 $(lat_1, lon_1)$，欧拉角pitch、roll、yaw 为 $(p_1,r_1,y_1)$，$F_2$ 对应时刻车体的经纬度为 $(lat_2, lon_2)$，欧拉角pitch、roll、yaw为 $(p_2,r_2,y_2)$。则$F_1$和$F_2$坐标系原点的球面距离计算公式如\ref{con:sphere_dis}所示，其中$R=6378137.0$米，为地球的半径。
\begin{equation}
d = 2R * \arcsin(\sqrt{\sin^2(\frac{lat_2 - lat_1}{2}) + \cos(lon_1)\cos(lon_2)\sin^2(\frac{lon_2 - lon_1}{2})})
\label{con:sphere_dis}
\end{equation}
为了将$F_2$的点云数据转换到$F_1$坐标系下，需要求得两坐标系的位移$\Delta$以及旋转矩阵$\mathcal{M}$。根据计算的两坐标原点的球面距离以及各自的欧拉角，位移$\Delta$计算如公式\ref{con:origin_offset}所示，旋转矩阵$\mathcal{M} = R_Z * R_X * R_Y$，计算如公式\ref{con:rotate_matrix}所示，其中$\delta_p = (p_2 - p_1)$，$\delta_r = (r_2 - r_1)$，$\delta_y = (y_2 - y_1)$，为两帧车体欧拉角的偏差。 假设$F_2$中所有点云的原始坐标矩阵为$P_{origin} = [X,Y,Z]^T \in \mathbb{R}^{ n \times 3}$，则其转换到$F_1$坐标系下的坐标矩阵$P_{trans} = (P_{origin} + \Delta) * \mathcal{M}$。坐标系转换前后两帧点云数据叠加可视化对比如下图所示，由图可知坐标系转换有利于两帧信息的关联。 
\begin{equation}
\Delta = [\delta_x, \delta_y, \delta_z]^T = [d\cos(y_2-y_1), d\sin(y_2-y_1),d\sin(p_2-p_1)]
\label{con:origin_offset}
\end{equation}
\begin{equation}
\mathcal{M} = \\
	\begin{bmatrix} 
		\cos(\delta_p) & 0 & \sin(\delta_p)\\
		0 & 1 & 0 \\
		-\sin(\delta_p) & 0 & \cos(\delta_p)
	\end{bmatrix}
	\begin{bmatrix} 
	1 & 0 & 0\\
	0 & \cos(\delta_r) & -sin(\delta_r) \\
	0 & sin(\delta_r) & \cos(\delta_r)
	\end{bmatrix}
	\begin{bmatrix} 
	\cos(\delta_y) & -\sin(\delta_y) & 0\\
	\sin(\delta_y) & \cos(\delta_y) & 0 \\
	0 & 0 & 1
	\end{bmatrix}
\label{con:rotate_matrix}
\end{equation}

在数据可视化时，我们注意到KITTI官方提供的多目标追踪标签并不完备。例如，在图\ref{fig:examples}第一行中，我们可视化了训练数据集第0段视频的几帧数据以及官方提供的标签，可以看到有些目标（由红色虚线框圈出）在128帧中有标签，但是在118以及120帧中却没有标签，尽管这些目标在118帧以及120帧中都能很好识别。这种情况在整个训练数据集中都经常出现，因此，为了更好的衡量DODT模型的检测与追踪性能，我们手动添加了这些漏打的标签。

\section{模型训练}
\label{training}
我们的实验在一台配置有Telsa P100 的GPU以及Intel(R) Xeon(R) E5-2667 v4 @ 3.20GHz的32核CPU的服务器上进行。为了训练并验证DODT的性能，我们将KITTI提供的21段训练视频流分成两份，视频编号为奇数为训练集，偶数的为验证集。我们将官方标记的”Car“以及”Van“目标都当做”Car“，只做该类别的检测与追踪。网络训练过程中的大部分超参数设置我们都借鉴了AVOD\cite{ku2018joint}。具体而言，DODT在训练数据集中迭代次数为120K，批处理大小为1。DODT使用ADAM\cite{kingma2014adam}作为优化器，并且使用了初始值为0.0001，每迭代30K步以0.8的因子指数下降的可变学习率。网络的损失值由四部分组成，目标检测的分类和回归损失以及时序信息模块的分类与回归损失，如公式\ref{con:total_loss}所示。其中训练时权重$w_{cls}, w_{reg}, w_{co}, w_{corr}$的值分别为1.0，5.0，1.0，1.0。RPN的参数设置以及NMS算法的阈值设置在第三章方法部分有详细介绍，这里不再赘述。
\begin{equation}
L_{total} = w_{cls}L_{cls} + w_{reg}L_{reg} + w_{co}L_{co} + w_{corr}L_{corr}
\label{con:total_loss}
\end{equation}

\input{figures/chap04_examples.tex}

\section{实验结果分析}
\label{results}



\subsection{三维物体检测结果分析}
\label{ablation_study}

\input{tables/shared_rpn_cmp.tex}

\input{tables/detection_cmp.tex}


\subsection{流数据检测结果分析}
\label{stream_result}


\subsection{多目标跟踪实验分析}
\label{mot_result}

\input{tables/tracking_local_cmp.tex}

\input{tables/tracking_benchmark_cmp.tex}


\section{结果展示}
\label{show}

\section{本章总结}
\label{exp_conclusion}


% 打印时插入必要的空白页
\ifprint
	\newpage
	\thispagestyle{empty}
	\mbox{}
	
	% 避免空白页影响页码编号
	\clearpage
	\setcounter{page}{10}
\fi