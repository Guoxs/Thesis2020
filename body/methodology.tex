% !Mode:: "TeX:UTF-8"

\chapter{流数据的三维目标检测与跟踪}
\label{methodology}
本章将详细介绍本工作提出的流数据三维目标检测与跟踪框架 \textbf{DODT} （\textbf{D}ual-way \textbf{O}bject \textbf{D}etection and \textbf{T}racking）。本章先介绍DODT的整体网络架构，然后再重点分析各个模块的功能以及实现。

\section{框架整体结构}
\label{total_structure}
\input{figures/chap03_dodt.tex}

DODT网络是双路结构，其结构如\figurename \, \ref{fig:dodt} 所示。DODT由四个小模块组成：三维物体检测模块、Shared RPN模块、时序信息处理模块以及运动插值模块。三维物体检测模块有两个分支，分别负责检测两相邻关键帧中的物体，两分支的网络结构相同，并且共享参数。检测模块的网络结构是基于AVOD\cite{ku2018joint}构建的，为二阶段物体检测框架的三维扩展，该模块的详细结构将在3.2节介绍。Shared RPN 模块负责生成二阶段物体检测中的候选框，与传统RPN不同的是，Shared RPN生成的3D候选框可以被两个三维检测分支共同使用，该模块将在3.3节详细介绍。时序处理模块为 \figurename \, \ref{fig:dodt} 的浅黄色区域，该模块通过对相邻关键帧的点云俯视图（Bird Eye View，BEV）特征匹配块进行相关（correlation） 操作提取帧间的时序信息，然后预测同一物体物体在两关键帧的位置位置偏移。3.4节将详细介绍时序处理模块的实现原理。运动插值模块主要是由独立于网络结构的运动插值算法构成，该算法使用三维物体检测模块对两关键帧的检测结果以及时序处理模块输出的位置偏移信息，将关键帧的检测结果传播到非关键帧，实现流数据的三维物体检测与追踪。该算法的详细流程将会在3.5节重点介绍。


\section{三维物体检测模块}
\label{3d_detection_module}

DODT框架的三维物体检测模块融合了点云与图像信息，预测自动驾驶场景中车辆的三维位姿信息。由于该检测模块是基于AVOD\cite{ku2018joint}网络，为二阶段物体检测模块，因此包含候选框提取网络。由于本框架的候选框提取网络与AVOD有所不同，因此将在下一节详细介绍。本节不展开介绍候选框提取环节。本节将依次介绍数据预处理、特征提取、RoI池化、特征融合以及最终的预测框生成等物体检测中的关键步骤。

\subsection{数据预处理}

DODT融合RGB图像数据以及激光雷达数据进行三维物体检测，因此每一帧数据都同时包含图像以及点云。RGB图像数据预处理比较简单，首先统一调整长宽到$1200 \times 360 $ px，然后在RGB通道上将去数据集的RGB平均值$[R_{mean},G_{mean},B_{mean}]$（本工作基于的KITTI下的物体追踪数据集，RGB平均值为[92.84,97.80,93.58]）。RGB图像数据经过色彩均值归一化后就可以直接用于后续的特征提取操作了。

点云数据的预处理相对于图像要更为复杂，这是因为点云是稀疏的三维数据，需要经过额外的操作将其转换为网络能够处理的形式。本工作采用网格化的方法将点云数据编码成六通道的BEV特征图，特征图包含五个高度切片通道以及一个点云密度信息通道。我们首先将点云投影到相机平面，然后过滤落在图像尺寸之外的点以便让点云视野与图像视野相同【TODO】。对于截取得到的规则三维视野，我们首先使用0.1m分辨率的网格将XZ平面网格化。假设XZ平面尺寸为 $[-W,W] \times [0, L] m$，则网格化后的尺寸为$\frac{2W}{0.1} \times \frac{L}{0.1}$。对于高度Y方向，我们截取$[0,2.5] m$ 的区域，然后平均切片为五等份从而将整个点云体素化。而后，将每个体素格子中所有点高度的最大值作为该格子的值，从而将点云编码为BEV特征图。对于特征图的最后一个通道，我们使用高度切片前的每个格子内的点计算该格子的点云密度 $\rho = \min(1.0, \frac{\log(N+1)}{\log 16})$，其中$N$为格子内点的数目。该方法最先在MV3D\cite{chen2017multi}中使用，而后AVOD\cite{ku2018joint}也使用了相同的处理方式。

在自动驾驶场景中，车辆是不停运动的，因此流数据中每一帧数据的参考系也不同。为了更好的关联帧与帧之间的时序信息，除了对每一帧数据进行预处理外，还需将不同帧的数据校准到同一坐标系。DODT为双路结构，同时处理相邻两帧关键帧数据并关联帧间信息。因此，为了统一两帧数据的坐标系，我们将后一帧关键帧数据校准到前一帧数据的坐标系中。由于图像数据没有准确的三维坐标信息，我们只校准点云数据。点云数据的校准需要用到激光雷达的位姿信息，这些数据KITTI数据集有提供。

\subsection{特征提取}

\input{figures/chap03_feature_extractor.tex}

DODT检测模块有两个单独的特征提取网络，分别用于图像特征提取以及点云特征提取。这两个网络的结构类似，只是输入的尺寸不一样。特征提取网络采用的编码器-解码器（Encoder-Decoder）结构，包含了编码器和解码器两部分，如\figurename \ref{fig:feature_extractor} 所示。编码器基于VGG-16网络改造的，首先将网络的通道数减半，并在conv-4层截断。编码器输入尺寸为 $M \times N \times D$ 的图像数据或是点云BEV特征图，然后输出尺寸为 $\frac{M}{8} \times \frac{N}{8} \times D^*$ 的特征$F$。特征$F$能够表达高层次的语意信息，并且尺寸比输入小了八倍。KITTI数据集中行人在BEV视角平均大小为$0.8 \times 0.6$ 米，在BEV特征图上就是$8 \times 6$ 的像素区域（分辨率为0.1m）。在编码器八倍下采样后，行人在输出特征$F$中占据不到一个像素，这还是在不考虑卷积过程中感受野的放大所造成小物体占比缩小的情况。受特征金字塔网络（Feature Pyramid Network）的启发，AVOD设计了一个自底向上的解码器，解码器可以在几乎不增加运行时间的基础上，将特征$F$上采样到输入的尺寸大小。解码器将编码器的输出作为输入，然后生成尺寸为$M \times N \times D$ 的特征图。解码器的上采样是通过反卷积算子实现的，为了更好的补回编码器中下采样造成的细节损失，解码器每一层的输入还额外包含编码器对应层的特征，然后通过一个$3 \times 3$的卷积操作融合。最终的特征图在保留了对高层语意的表征能力下，还能有与输入相同的尺寸，这在一定程度上避免了小物体特征丢失的问题。

\subsection{特征融合}

\input{figures/chap03_fusion_network.tex}

在特征提取阶段我们分别得到了图像的特征$F_{img}$以及点云的特征$F_{pc}$，接下来就是将这两种特征融合以得到更丰富的视觉特征。DODT的特征融合也是借鉴了AVOD的方式，在候选框层面上进行融合。给定一个3D候选框由（Shared RPN生成），将其投影到$F_{img}$以及$F_{pc}$上，截取相应的部分就能得到对应的候选区域特征$f_{img}$ 以及$f_{pc}$。之后$f_{img}$ 和 $f_{pc}$ 将会通过$7 \times 7$的ROI池化操作生成相同尺寸的特征向量，之后两特征向量通过一个融合网络生成最终的候选区域特征向量$F_{fusion}$。融合网络结构如\figurename \ref{fig:fusion_network} 所示，该网络结构最先由MV3D提出，叫做深度融合网络。对于有$L$层的网络，深度融合网络按如下方式融合特征：
\begin{equation}
	\begin{aligned}
		& F_0 = F_{img} \oplus F_{pc}\\
		& F_l = H^{img}_l(F_{l-1}) \oplus H^{pc}_l(F_{l-1})\\
		& \forall l = 1, ..., L
	\end{aligned}
\label{con:deep_fusion}
\end{equation}
其中，$\{H_l, l=1,...,L\}$是特征转移函数（由神经网络拟合），$\oplus$为某种融合操作（例如拼接、求和等，DODT使用的是平均）。深度融合网络可以让两种特征能在更深层次、更高的语意层面进行融合，从而取得更好的融合效果。

\subsection{预测框生成}

\input{figures/chap03_box_encoding.tex}

特征融合之后，融合的特征图将会输入到两个不同任务分支，分别为检测分支和回归分支， 这两个分支都是由三层2048个神经元组成的全连接层。检测分支负责预测目标的类别，使用交叉熵计算损失值；而回归分支负责预测候选框与真实物体框的差值，使用$Smooth L1$ 函数计算损失值。在回归损失中，只有当候选框与真实框在BEV视角的 2D IoU 大于 0.65 才会被计算。之后， DODT使用NMS算法去除重叠的框，阈值设置为 0.01。

在框回归中，对三维框的编码有很多种方式，其中比较常见的有八点编码法和轴对齐（Axis Aligned） 编码法，如\figurename \ref{fig:box_encoding} 所示。八点编码法直接编码八个顶点的坐标，这种方式没有考虑三维框自身的几何约束，有一定的冗余性。而轴对齐编码限制了三维框要和坐标轴对齐，这是在RPN阶段使用的，并不适合最终的框编码。DODT使用了AVOD提出的10参数编码法，如\figurename \ref{fig:box_encoding}最右侧所示，分别编码底部四点坐标以及底面与顶面和地面的距离，一共有十个参数，因此框回归的目标为 $(\Delta_{x1}, ...,\Delta_{y1}, \Delta_{y4}, \Delta_{h1}, \Delta_{h2})$。这种编码方式考虑到了三维框顶面四点和底面四点是对齐的，而原来的过参数化的八点法需要编码成24维向量。

在MV3D中，作者默认三维预测框的朝向为长边方向，可是这种确定朝向的方法是有问题的。首先并不是所有目标都适用于这种确定朝向的方法，比如行人；其次，长边有两个可取的方向，它们相差 $\pm \pi$。AVOD中，作者通过预测转向向量 $(x_{\theta},y_{\theta}) = (\cos(\theta), \sin(\theta))$来解决这个问题。这种方式可以将每个转向角$\theta \in [-\pi, \pi]$都映射到唯一的转向向量。转向向量的预测也是包含在回归分支中。另外，转向向量可以用来解决十参数编码法中预测框与真实框底部四点的对应关系。底部四点的对应有四种，只使用十参数编码时只能通过最近匹配法，有了转向向量之后，转向向量的值可以作为额外的匹配手段。


\section{Shared RPN模块}
\label{shared_rpn}

\input{figures/chap03_rpn.tex}

\input{figures/chap03_integrated_boxes.tex}

\section{时序信息处理模块}
\label{temporal_module}

\begin{equation}
\delta^{t, t+\tau} = \\
\begin{cases}
(\frac{d_{x}^{t+\tau} - d_{x}^{t}}{d^t_{w}}, \frac{d_{z}^{t+\tau} - d_{z}^{t} }{d^t_{l}}, \frac{d_{ry}^{t+\tau} - d_{ry}^{t}}{d^t_{ry}}) & p_{co} = 1.0 \\
(0.0,0.0, 0.0) &  otherwise
\end{cases}
\end{equation}

\section{运动插值模块}
\label{interpolation}

\begin{algorithm}
	\caption{基于运动模型的插值算法(MoI Algorithm)}
	\label{alg:interpolation}
	\textbf{输入: } $D^t= [d^t_0, d^t_1, ..., d^t_{N_t}], D^{t+\tau}= [d^{t+\tau}_0, d^{t+\tau}_1, ..., d^{t+\tau}_{N_{t+\tau}}],$
	$\Delta^{t, t+\tau}=[\delta^{t, t+\tau}_0, \delta^{t, t+\tau}_1, ..., \delta^{t, t+\tau}_{N_{max}}], N_{max} = \max\{N_t, N_{t+\tau}\}$\\
	\textbf{输出: } $D = [D^t, D^{t+1}, ..., D^{t+\tau}]$\\
	\textbf{初始化:} $D_{temp} = D^{t+\tau}, D, p_{co}^{max} = 0.5$ \\
	\For{$d^t_i \emph{ in } D^t$}{
		$\Delta d^{t, t+ \tau}_{i} = (d^t_{i, w} \cdot \delta^{t, t+\tau}_{i,x}, 0, d^t_{i, l} \cdot \delta^{t, t+\tau}_{i,z}, 0, 0, d^t_{i, ry} \cdot \delta^{t, t+\tau}_{i,ry})$
		$d' = getMatched(d^t_i+\Delta d^{t, t+ \tau}_{i}, D_{temp})$\\
		\If{$d'$}{
			$d^{t+1}_i,..., d^{t+\tau-1}_i = Interpolate(d^t_i, d')$\\
			remove $d'$ from $D_{temp}$
		}
		\ElseIf{$p_{co}^i \geq p_{co}^{max}$}{
			$d^{t+1}_i,..., d^{t+\tau}_i = Interpolate(d^t_i, d^t_i + \Delta d^{t, t+ \tau}_{i})$
		}
		\Else{generate $(d^{t+1}_i,..., d^{t+\tau-1}_i)$ by motion model}
	}
	\If{$D_{temp}$ is not empty}{
		\For{$d^{t+\tau}_j \emph{ in } D_{temp}$}{
			\If{$p_{co}^j \geq p_{co}^{max}$}{
				$d^{t}_j,..., d^{t+\tau-1}_j = Interpolate(d^{t+\tau}_j - \Delta d^{t, t+ \tau}_{j}, d^{t+\tau}_j)$
			}
			\Else{
				generate $(d^{t+1}_j,..., d^{t+\tau-1}_j)$ by motion model
			}
		}
	}
\end{algorithm}



\section{多目标追踪}
\label{tracking_module}




\section{本章总结}
\label{metho_conclusion}

% 打印时插入必要的空白页
\ifprint
\newpage
\thispagestyle{empty}
\mbox{}

% 避免空白页影响页码编号
\clearpage
\setcounter{page}{10}
\fi